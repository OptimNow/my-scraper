# Checklist Pr√©-D√©ploiement

## ‚úÖ Tests Locaux

### 1. Installation et V√©rification
```bash
# Installer les d√©pendances
[ ] npm install

# V√©rifier que package-lock.json est cr√©√©
[ ] ls package-lock.json

# V√©rifier la syntaxe
[ ] node -c api/scrape_hub.js
[ ] node -c scripts/run_scrape_batch.js
```

### 2. Configuration Locale
```bash
# Copier le fichier d'exemple
[ ] cp .env.example .env

# √âditer .env avec vos vraies credentials
[ ] nano .env  # ou votre √©diteur pr√©f√©r√©

# V√©rifier que .env n'est PAS track√© par git
[ ] git status | grep -q ".env" && echo "ERREUR: .env est track√©!" || echo "OK"
```

### 3. Test de l'API en Local
```bash
# D√©marrer le serveur de d√©veloppement Vercel
[ ] vercel dev
# (doit afficher "Ready! Available at http://localhost:3000")

# Dans un autre terminal, tester une requ√™te simple
[ ] curl "http://localhost:3000/api/scrape_hub?limit=1" | jq

# V√©rifier la r√©ponse JSON contient:
[ ] - requestId
[ ] - count
[ ] - items array
[ ] - logging structur√© visible dans les logs du serveur
```

### 4. Test du Script Batch
```bash
# S'assurer que le serveur local tourne
[ ] (vercel dev dans un terminal)

# Configurer les variables d'environnement
[ ] export AWS_ACCESS_KEY_ID=xxx
[ ] export AWS_SECRET_ACCESS_KEY=xxx
[ ] export S3_BUCKET_NAME=your-test-bucket
[ ] export SCRAPER_API_URL=http://localhost:3000/api/scrape_hub

# Tester avec limite de 1
[ ] node scripts/run_scrape_batch.js --limit=1

# V√©rifier les fichiers dans S3
[ ] aws s3 ls s3://your-test-bucket/Recos/
```

---

## üöÄ D√©ploiement Vercel

### 5. Configuration Vercel
```bash
# Se connecter √† Vercel (si pas d√©j√† fait)
[ ] vercel login

# Lier le projet (si pas d√©j√† fait)
[ ] vercel link
```

### 6. Variables d'Environnement Vercel
Dans le dashboard Vercel (https://vercel.com/dashboard):

[ ] Aller dans Project Settings > Environment Variables
[ ] Ajouter AWS_ACCESS_KEY_ID (Production)
[ ] Ajouter AWS_SECRET_ACCESS_KEY (Production)
[ ] Ajouter S3_BUCKET_NAME (Production)
[ ] Ajouter AWS_REGION (Production, optionnel)
[ ] Ajouter S3_PREFIX (Production, optionnel)

### 7. D√©ploiement
```bash
# D√©ployer en production
[ ] vercel --prod

# Noter l'URL de d√©ploiement
[ ] URL: _________________________________

# Attendre la fin du build (environ 1-2 minutes)
```

### 8. Tests de Production
```bash
# Tester l'endpoint de production
[ ] curl "https://your-app.vercel.app/api/scrape_hub?limit=1" | jq

# V√©rifier la r√©ponse contient des donn√©es valides
[ ] Pas d'erreur 500
[ ] JSON bien form√©
[ ] items array non vide

# Tester avec une URL sp√©cifique
[ ] curl "https://your-app.vercel.app/api/scrape_hub?url=https://hub.pointfive.co/hub/inefficiencies/..." | jq
```

### 9. Test du Batch Script en Production
```bash
# Configurer l'URL de production
[ ] export SCRAPER_API_URL=https://your-app.vercel.app/api/scrape_hub
[ ] export AWS_ACCESS_KEY_ID=xxx
[ ] export AWS_SECRET_ACCESS_KEY=xxx
[ ] export S3_BUCKET_NAME=your-prod-bucket

# Lancer un petit batch de test
[ ] node scripts/run_scrape_batch.js --limit=2

# V√©rifier les fichiers dans S3 de production
[ ] aws s3 ls s3://your-prod-bucket/Recos/ --recursive | tail -10
```

---

## üìä V√©rifications Post-D√©ploiement

### 10. Monitoring des Logs
```bash
# Voir les logs Vercel
[ ] vercel logs --follow

# V√©rifier qu'il n'y a pas d'erreurs
[ ] Rechercher "error" dans les logs
[ ] V√©rifier que le logging structur√© fonctionne (JSON)
```

### 11. V√©rification des Donn√©es S3
```bash
# T√©l√©charger un fichier de test
[ ] aws s3 cp s3://your-bucket/Recos/pointfive_xxx.json test.json

# V√©rifier le contenu
[ ] cat test.json | jq

# V√©rifier les champs requis
[ ] cat test.json | jq 'has("id", "title", "source", "scraped_at")'
```

### 12. Tests de Charge (Optionnel)
```bash
# Tester avec limite plus √©lev√©e
[ ] curl "https://your-app.vercel.app/api/scrape_hub?limit=5"

# V√©rifier que √ßa ne timeout pas (< 10s sur free tier)
[ ] Mesurer le temps de r√©ponse

# Si timeout, r√©duire la limite par d√©faut davantage
```

---

## üõ°Ô∏è S√©curit√©

### 13. V√©rifications de S√©curit√©
[ ] .env est dans .gitignore et NON commit√©
[ ] Les credentials AWS ont les permissions minimales (S3 PutObject seulement)
[ ] Le bucket S3 n'est pas public
[ ] Les logs ne contiennent PAS de credentials
[ ] User-Agent est appropri√© (pas example.com)

---

## üìù Documentation

### 14. Mise √† Jour de la Documentation
[ ] README.md est √† jour avec les nouvelles features
[ ] Variables d'environnement document√©es
[ ] Exemples de commandes test√©s et fonctionnels
[ ] Limites connues document√©es (timeout Vercel, etc.)

---

## ‚úÖ Validation Finale

Si toutes les cases sont coch√©es:

- ‚úÖ Code fonctionne localement
- ‚úÖ Code fonctionne en production
- ‚úÖ Donn√©es arrivent correctement dans S3
- ‚úÖ Logging structur√© fonctionne
- ‚úÖ Gestion d'erreurs test√©e
- ‚úÖ Pas de secrets expos√©s
- ‚úÖ Documentation √† jour

**üéâ VOTRE SCRAPER EST PR√äT POUR LA PRODUCTION ! üéâ**

---

## üÜò Troubleshooting

### Probl√®mes Courants

**Erreur: "Missing required environment variables"**
- Solution: V√©rifier que AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY, et S3_BUCKET_NAME sont d√©finis

**Erreur: "Fetch timeout after 8000ms"**
- Solution: Le site hub.pointfive.co est peut-√™tre lent, augmenter FETCH_TIMEOUT_MS dans api/scrape_hub.js

**Erreur: "Vercel Function Timeout (10s)"**
- Solution: R√©duire la limite par d√©faut (actuellement 5), ou upgrader vers Vercel Pro (60s timeout)

**Erreur: "Access Denied" lors de l'upload S3**
- Solution: V√©rifier les permissions IAM de votre utilisateur AWS

**Pas de donn√©es dans S3**
- Solution: V√©rifier les logs Vercel, v√©rifier BUCKET_NAME, v√©rifier les credentials AWS

---

## üìû Support

- Documentation Vercel: https://vercel.com/docs
- Documentation AWS S3: https://docs.aws.amazon.com/s3/
- Issues GitHub: Cr√©er une issue dans votre repo
